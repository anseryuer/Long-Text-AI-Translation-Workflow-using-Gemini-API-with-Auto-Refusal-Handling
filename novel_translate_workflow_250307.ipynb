{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0LDWCms7afu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "import re\n",
        "# Configure API key (replace with your key)\n",
        "genai.configure(api_key=\"\")\n",
        "\n",
        "# Model setup\n",
        "generation_config = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 1,\n",
        "    \"max_output_tokens\": 4096,\n",
        "}\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a multilingual translator. You will be given a text in the input language. Your task is to translate the text into the output language. \"\n",
        "    \"You must provide a translation that is accurate, consistent, and fluent in the output language. The output must only be the translation of the text, with no explanation.\"\n",
        ")\n",
        "safety_settings = {\n",
        "    \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": \"BLOCK_NONE\",\n",
        "    \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"BLOCK_NONE\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.0-flash\",  # Adjust model name as needed\n",
        "    generation_config=generation_config,\n",
        "    system_instruction=system_prompt,\n",
        "    safety_settings=safety_settings\n",
        ")\n",
        "\n",
        "def generate_glossary(text, max_retries=3, min_chunk_size=500):\n",
        "    \"\"\"\n",
        "    Generate a glossary from the text, handling API rejections by splitting recursively.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to extract terms from.\n",
        "        max_retries (int): Maximum retries before giving up on a chunk.\n",
        "        min_chunk_size (int): Minimum chunk size to avoid infinite splitting.\n",
        "\n",
        "    Returns:\n",
        "        dict: Glossary mapping Japanese terms to Simplified Chinese translations.\n",
        "    \"\"\"\n",
        "    glossary_prompt = (\n",
        "        \"You are a multilingual glossary generator. I will provide a text in the input language. You will generate a glossary for this text, listing special terms that require consistent translation, along with their translations in the output language.\"\n",
        "        \"Extract special terms (single words only) from the following text, such as names of characters, places, unique items, etc., which may not have a single correct translation and could be inconsistent across multiple translations.\"\n",
        "        \"Provide the special terms in the input language and their most appropriate translations in the output language. Each term should correspond to only one translation. The output should be a list of input language terms mapped to their output language translations.\"\n",
        "        \"Output format: term1: translation1\\nterm2: translation2\\n...\\n\\n\"\n",
        "    )\n",
        "    def process_chunk(chunk, retries_left):\n",
        "        full_prompt = glossary_prompt + chunk\n",
        "        try:\n",
        "            response = model.generate_content([full_prompt])\n",
        "            glossary = {}\n",
        "            for line in response.text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    term, translation = line.split(':', 1)\n",
        "                    glossary[term.strip()] = translation.strip()\n",
        "            return glossary, True\n",
        "        except Exception as e:\n",
        "            if retries_left <= 0:\n",
        "                print(f\"Failed to process chunk after {max_retries} retries: {e}\")\n",
        "                return {}, False\n",
        "            print(f\"Error processing chunk: {e}. Retrying with split...\")\n",
        "            time.sleep(2)\n",
        "\n",
        "            if len(chunk) <= min_chunk_size:\n",
        "                print(f\"Chunk too small to split further: {chunk[:20]}...\")\n",
        "                return {}, False\n",
        "\n",
        "            mid = len(chunk) // 2\n",
        "            paragraphs = chunk.split('\\n')\n",
        "            if len(paragraphs) > 1:\n",
        "                split_idx = sum(len(p) for p in paragraphs[:len(paragraphs)//2]) + 1\n",
        "                first_half = chunk[:split_idx]\n",
        "                second_half = chunk[split_idx:]\n",
        "            else:\n",
        "                first_half = chunk[:mid]\n",
        "                second_half = chunk[mid:]\n",
        "\n",
        "            glossary1, success1 = process_chunk(first_half, retries_left - 1)\n",
        "            glossary2, success2 = process_chunk(second_half, retries_left - 1)\n",
        "            merged_glossary = glossary1.copy()\n",
        "            for term, translation in glossary2.items():\n",
        "                if term not in merged_glossary:\n",
        "                    merged_glossary[term] = translation\n",
        "            return merged_glossary, success1 or success2\n",
        "\n",
        "    glossary, success = process_chunk(text, max_retries)\n",
        "    if not success:\n",
        "        print(\"Warning: Glossary generation partially failed. Some terms may be missing.\")\n",
        "    return glossary\n",
        "\n",
        "def generate_summary(text, glossary_str):\n",
        "    \"\"\"\n",
        "    Generate a brief Chinese summary of the text using the glossary for consistency.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to summarize.\n",
        "        glossary_str (str): Glossary string for term consistency.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated summary in Chinese.\n",
        "    \"\"\"\n",
        "    summary_prompt = (\n",
        "        \"You are a multilingual text summarizer. I will provide a text in the input language. You will summarize this text briefly in the output language.\"\n",
        "        f\"Using the following glossary {glossary_str}, summarize the following text {text} in no more than 3 sentences. Only output the summary in the output language.\"\n",
        "    )\n",
        "    try:\n",
        "        response = model.generate_content([summary_prompt])\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating summary: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def contains_japanese(text, threshold=0.25):\n",
        "    \"\"\"\n",
        "    Check if Japanese characters exceed a threshold percentage of the text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to check.\n",
        "        threshold (float): Max allowed proportion of Japanese characters (default 0.25).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if Japanese exceeds threshold, False otherwise.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "    japanese_count = sum(1 for char in text if '\\u3040' <= char <= '\\u309F' or '\\u30A0' <= char <= '\\u30FF')\n",
        "    return (japanese_count / len(text)) >= threshold\n",
        "\n",
        "def translate_with_splitting(fixed_prompt, chunk, min_chunk_size=500):\n",
        "    \"\"\"\n",
        "    Translate a chunk, splitting recursively if the API rejects it.\n",
        "\n",
        "    Args:\n",
        "        fixed_prompt (str): Fixed part of the prompt (summary, glossary, context).\n",
        "        chunk (str): Text chunk to translate.\n",
        "        min_chunk_size (int): Minimum chunk size.\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text.\n",
        "    \"\"\"\n",
        "    prompt = fixed_prompt + f\"Translate the following Japanese text to Chinese: {chunk}\"\n",
        "\n",
        "    def try_translate():\n",
        "        try:\n",
        "            response = model.generate_content([prompt])\n",
        "            translated = response.text\n",
        "            if not contains_japanese(translated):\n",
        "                return translated\n",
        "            print(\"Too much Japanese detected, retrying...\")\n",
        "            time.sleep(1)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            time.sleep(10)\n",
        "            return None\n",
        "\n",
        "    translated = try_translate()\n",
        "    if translated is not None:\n",
        "        return translated\n",
        "\n",
        "    if len(chunk) <= min_chunk_size:\n",
        "        print(f\"Chunk too small to split further: {chunk[:20]}...\")\n",
        "        return \"\"\n",
        "\n",
        "    mid = len(chunk) // 2\n",
        "    first_half = chunk[:mid]\n",
        "    second_half = chunk[mid:]\n",
        "\n",
        "    translated_first = translate_with_splitting(fixed_prompt, first_half)\n",
        "    translated_second = translate_with_splitting(fixed_prompt, second_half)\n",
        "    return translated_first + translated_second\n",
        "\n",
        "def split_into_chunks(text, chunk_size=3192, overlap=0):\n",
        "    \"\"\"\n",
        "    Split text into chunks with optional overlap.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to split.\n",
        "        chunk_size (int): Size of each chunk.\n",
        "        overlap (int): Overlap between chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "def translate_long_text(text, chunk_size=3192, context_mode='chinese_only', context_lines=10):\n",
        "    \"\"\"\n",
        "    Translate long text with glossary, summary, and context options.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to translate.\n",
        "        chunk_size (int): Size of each chunk.\n",
        "        context_mode (str): 'jp_ch', 'chinese_only', or 'no_context'.\n",
        "        context_lines (int): Number of context lines.\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text.\n",
        "    \"\"\"\n",
        "    glossary = generate_glossary(text)\n",
        "    glossary = {term: translation for term, translation in glossary.items() if len(term) <= 25}\n",
        "    glossary_str = \", \".join([f\"'{term}': '{translation}'\" for term, translation in glossary.items()])\n",
        "    print(f\"Generated glossary: {glossary_str}\")\n",
        "    summary = generate_summary(text, glossary_str)\n",
        "    print(f\"Generated summary: {summary}\")\n",
        "\n",
        "    chunks = split_into_chunks(text, chunk_size, overlap=0)\n",
        "    translated_chunks = []\n",
        "    prev_jp_lines = []\n",
        "    prev_ch_lines = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        filtered_glossary = {term: translation for term, translation in glossary.items() if term in chunk}\n",
        "        filtered_glossary_str = \", \".join([f\"'{term}': '{translation}'\" for term, translation in filtered_glossary.items()])\n",
        "\n",
        "        fixed_prompt = f\"Summary: {summary}\\n\\n\"\n",
        "        if context_mode == 'jp_ch' and (prev_jp_lines or prev_ch_lines):\n",
        "            jp_context = '\\n'.join(prev_jp_lines[-context_lines:]) if prev_jp_lines else \"\"\n",
        "            ch_context = '\\n'.join(prev_ch_lines[-context_lines:]) if prev_ch_lines else \"\"\n",
        "            fixed_prompt += f\"Previous Japanese context: {jp_context}\\nPrevious Chinese context: {ch_context}\\n\\n\"\n",
        "        elif context_mode == 'chinese_only' and prev_ch_lines:\n",
        "            ch_context = '\\n'.join(prev_ch_lines[-context_lines:])\n",
        "            fixed_prompt += f\"Previous context: {ch_context}\\n\\n\"\n",
        "        # 'no_context' adds nothing to fixed_prompt beyond summary\n",
        "\n",
        "        fixed_prompt += f\"Glossary: {filtered_glossary_str}\\n\\n\"\n",
        "        translated = translate_with_splitting(fixed_prompt, chunk)\n",
        "        translated_chunks.append(translated)\n",
        "\n",
        "        # Update context\n",
        "        jp_lines = chunk.split('\\n')\n",
        "        ch_lines = translated.split('\\n')\n",
        "        prev_jp_lines.extend(jp_lines)\n",
        "        prev_ch_lines.extend(ch_lines)\n",
        "\n",
        "    return \"\".join(translated_chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OEK8q9b7afw"
      },
      "outputs": [],
      "source": [
        "def reduce_repetitive_characters(text):\n",
        "    processed_text = re.sub(r'(.{3})\\1{2,}', r'\\1\\1', text, flags=re.DOTALL)\n",
        "    processed_text = re.sub(r'(.{2})\\1{2,}', r'\\1\\1', processed_text, flags=re.DOTALL)\n",
        "    processed_text = re.sub(r'(.)\\1{2,}', r'\\1\\1', processed_text, flags=re.DOTALL)\n",
        "    return processed_text\n",
        "\n",
        "# Move file to finished folder\n",
        "def move_file_to_finished(file_path):\n",
        "    finished_path = os.path.join(\"finished_workflow\", os.path.basename(file_path))\n",
        "    try:\n",
        "        os.rename(file_path, finished_path)\n",
        "    except FileExistsError:\n",
        "        base_name, extension = os.path.splitext(finished_path)\n",
        "        new_path = f\"{base_name}_2{extension}\"\n",
        "        os.rename(file_path, new_path)\n",
        "        print(f\"Renamed to {new_path} due to existing file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZyhK7YH7afw",
        "outputId": "9b0eed53-737a-4beb-b233-c88c48cc06cc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Process all .txt files\n",
        "preprocessing = True\n",
        "for file in os.listdir():\n",
        "    if not file.endswith(\".txt\"):\n",
        "        continue\n",
        "    print(f\"Processing {file}...\")\n",
        "    try:\n",
        "        # Try utf-16 first\n",
        "        with open(file, 'r', encoding='utf-16') as f:\n",
        "            text = f.read()\n",
        "    except UnicodeError:\n",
        "        try:\n",
        "        # Try utf-8 next\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "        except UnicodeError:\n",
        "        # Fall back to default encoding with errors handled\n",
        "            with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                text = f.read()\n",
        "            print(f\"Warning: Encoding issues detected in {file}, using replacement characters\")\n",
        "    if True:\n",
        "        if preprocessing:\n",
        "            text = reduce_repetitive_characters(text)\n",
        "        translated_text = translate_long_text(text)\n",
        "        output_file = f\"translated_workflow/{file[:-4]}_translated.txt\"\n",
        "        os.makedirs(\"translated_workflow\", exist_ok=True)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(translated_text)\n",
        "        print(f\"{file} translated to {output_file}\")\n",
        "        move_file_to_finished(file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
